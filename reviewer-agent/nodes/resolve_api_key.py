"""
Node: resolve_api_key

Determines which LLM API key to use for the review:
  1. Poster's key (if under max_reviews limit)
  2. Freelancer's key (fallback)
  3. Platform default key (env var fallback for testing)
  4. None — skip automated review

Calls GET /api/v1/tasks/:id/review-config to get the resolved key.
"""

from __future__ import annotations
import os
import httpx
from state import ReviewerState


def resolve_api_key(state: ReviewerState) -> dict:
    """Resolve which LLM API key to use for this review."""
    if state.get("error") and state.get("skip_review"):
        return {}

    if state.get("error"):
        return {}

    task_id = state["task_id"]
    base_url = os.environ["TASKHIVE_BASE_URL"]
    api_key = os.environ["TASKHIVE_REVIEWER_API_KEY"]

    try:
        resp = httpx.get(
            f"{base_url}/api/v1/tasks/{task_id}/review-config",
            headers={"Authorization": f"Bearer {api_key}"},
            timeout=10.0,
        )
        resp.raise_for_status()
        data = resp.json()
    except httpx.HTTPError as exc:
        # Fall back to platform default key
        default_key = _get_default_key()
        if default_key:
            return {
                "llm_api_key": default_key["key"],
                "llm_provider": default_key["provider"],
                "llm_model": default_key["model"],
                "key_source": "none",
                "poster_reviews_used": 0,
                "poster_max_reviews": None,
            }
        return {
            "error": f"Failed to fetch review config and no default key available: {exc}",
            "skip_review": True,
        }

    if not data.get("ok"):
        default_key = _get_default_key()
        if default_key:
            return {
                "llm_api_key": default_key["key"],
                "llm_provider": default_key["provider"],
                "llm_model": default_key["model"],
                "key_source": "none",
                "poster_reviews_used": 0,
                "poster_max_reviews": None,
            }
        return {"skip_review": True, "key_source": "none"}

    config = data["data"]
    resolved_key = config.get("resolved_key")
    key_source = config.get("key_source", "none")

    # If API resolved a key (poster or freelancer), use it
    if resolved_key:
        provider = config.get("resolved_provider") or "openrouter"
        model = _model_for_provider(provider)
        return {
            "llm_api_key": resolved_key,
            "llm_provider": provider,
            "llm_model": model,
            "key_source": key_source,
            "poster_reviews_used": config.get("poster_reviews_used", 0),
            "poster_max_reviews": config.get("poster_max_reviews"),
        }

    # No key from API — try platform default
    default_key = _get_default_key()
    if default_key:
        return {
            "llm_api_key": default_key["key"],
            "llm_provider": default_key["provider"],
            "llm_model": default_key["model"],
            "key_source": "none",
            "poster_reviews_used": config.get("poster_reviews_used", 0),
            "poster_max_reviews": config.get("poster_max_reviews"),
        }

    # No key available — skip automated review
    print(f"  [resolve_api_key] No LLM key available for task {task_id} — skipping automated review")
    return {
        "skip_review": True,
        "key_source": "none",
        "poster_reviews_used": config.get("poster_reviews_used", 0),
        "poster_max_reviews": config.get("poster_max_reviews"),
    }


def _get_default_key() -> dict | None:
    """Get the platform default LLM key from environment variables."""
    if key := os.environ.get("OPENROUTER_API_KEY"):
        model = os.environ.get("DEFAULT_LLM_MODEL", "anthropic/claude-haiku-4-5-20251001")
        return {"key": key, "provider": "openrouter", "model": model}
    if key := os.environ.get("ANTHROPIC_API_KEY"):
        model = os.environ.get("DEFAULT_LLM_MODEL", "claude-haiku-4-5-20251001")
        return {"key": key, "provider": "anthropic", "model": model}
    if key := os.environ.get("OPENAI_API_KEY"):
        model = os.environ.get("DEFAULT_LLM_MODEL", "gpt-4o-mini")
        return {"key": key, "provider": "openai", "model": model}
    return None


def _model_for_provider(provider: str) -> str:
    """Return the default model for a given provider."""
    defaults = {
        "openrouter": os.environ.get("DEFAULT_LLM_MODEL", "anthropic/claude-haiku-4-5-20251001"),
        "openai": "gpt-4o-mini",
        "anthropic": "claude-haiku-4-5-20251001",
    }
    return defaults.get(provider, "anthropic/claude-haiku-4-5-20251001")
